{
  "timeout_s": 60,
  "threads": 4,
  "stats_file": "stats.json",
  "iteration_limit": 10,
  "llm_config": {
    "provider": "Ollama",
    "model": "llama3.1",
    "temperature": 0.3,
    "max_input_tokens": 4096,
    "max_output_tokens": 4096
  }
}
