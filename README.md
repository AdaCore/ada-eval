# Ada Eval
Framework for evaluating LLM based tools for Ada/SPARK use cases.

- [Ada Eval](#ada-eval)
  - [Setup](#setup)
    - [Install uv](#install-uv)
    - [ANCR Setup](#ancr-setup)
    - [Alire Setup](#alire-setup)
    - [Manual Setup](#manual-setup)
    - [Per-clone setup](#per-clone-setup)
  - [Running the Project](#running-the-project)
  - [Project Development Info](#project-development-info)
    - [Project structure](#project-structure)
    - [Common Commands](#common-commands)
    - [Adding a new Sample (Challenge/Problem)](#adding-a-new-sample-challengeproblem)
      - [Adding a new SPARK sample](#adding-a-new-spark-sample)
    - [Generating new completions](#generating-new-completions)
    - [Evaluating completions](#evaluating-completions)
    - [Adding or Updating Python Dependencies](#adding-or-updating-python-dependencies)
    - [The package version is already in `eng/ai/ada-eval` or `it/package-registry`](#the-package-version-is-already-in-engaiada-eval-or-itpackage-registry)
    - [The package version is not in `eng/ai/rag` or `it/package-registry`](#the-package-version-is-not-in-engairag-or-itpackage-registry)


## Setup

To get started with this project, you will need the following tools installed on your system:
- [uv](https://docs.astral.sh/uv/)
- [e3-cli](https://gitlab.adacore-it.com/it/e3-cli/)
- [git](https://git-scm.com/)
- [make](https://www.gnu.org/software/make/)

You will also need an Ada toolchain and GNATprove installed on your system. Some options for doing this:
1. [ANCR](#ancr-setup)
2. [Alire](#alire-setup)
3. [Manual](#manual-setup)

### Install uv

uv should not be installed in the project's virtual environment. We can use pipx to install it globally, but still in a virtual environment. For alternative methods, see the [official docs](https://docs.astral.sh/uv/getting-started/installation/).

```sh
# Using official script:
curl -LsSf https://astral.sh/uv/install.sh | sh

# Using pipx
pipx install uv

# Using brew
brew install uv

# Using cargo
cargo install --git https://github.com/astral-sh/uv uv
```

uv can update itself by running:
```sh
uv self update
```

Optionally, you can also install completions for uv and uvx:

```sh
# Determine your shell (e.g., with `echo $SHELL`), then run one of:

# For bash
echo 'eval "$(uv generate-shell-completion bash)"' >> ~/.bashrc
echo 'eval "$(uvx --generate-shell-completion bash)"' >> ~/.bashrc

# For zsh
echo 'eval "$(uv generate-shell-completion zsh)"' >> ~/.zshrc
echo 'eval "$(uvx --generate-shell-completion zsh)"' >> ~/.zshrc

# For fish
echo 'uv generate-shell-completion fish | source' >> ~/.config/fish/config.fish
echo 'uvx --generate-shell-completion fish | source' >> ~/.config/fish/config.fish

# For elvish
echo 'eval (uv generate-shell-completion elvish | slurp)' >> ~/.elvish/rc.elv
echo 'eval (uvx --generate-shell-completion elvish | slurp)' >> ~/.elvish/rc.elv

```

See [docs](https://docs.astral.sh/uv/) for more info about uv.

### ANCR Setup
Note that this won't work on macOS systems.

If you haven't already, you will need to clone [ANCR](https://github.com/AdaCore/ANCR). You should then run:
```
./bin/ancr all mcp
./bin/ancr shell mcp
code ../ada-eval
uv sync
```
You should now have a working Ada toolchain and GNATprove installed and in your environment.

### Alire Setup

If you're using macOS, you can use Alire to set up your environment by running:

```sh
alr install gnat_native gnatprove gprbuild
```

You must ensure that ~/.alire/bin/ is in your PATH.

### Manual Setup
Figure out how to install everything yourself ğŸ‘

### Per-clone setup

Do this when you first clone the project.
Use uv to install python dependencies (this will automatically create a virtual environment):

```sh
uv sync
```

By default, uv will create a virtual environment named .venv in your project. If you're using VS Code, you can use `Python: Select Interpreter` to select Python from the virtual environment generated by uv. This will enable code navigation.

If you don't use VS Code, you use the standard `source .venv/bin/activate` or equivalent to activate the virtual environment.

Alternatively, you can run commands in the project's virtual environment without activating it by running `uv run ...` (e.g., `uv run pytest`).


## Running the Project

The project has a simple CLI, as well as a makefile that contains some useful commands for your convenience.

You can see the CLI options by running:

```sh
uv run ada-eval --help
```

If you look at the make target `generate-spark-claude`, you will see an example of how to generate completions for our existing challenges, using Claude Code.
```sh
make generate-spark-claude
```

## Project Development Info

### Project structure

At a high level, the project is structured as follows:

```sh
.
â”œâ”€â”€ data  # Data directory containing various datasets
â”‚   â”œâ”€â”€ base  # Base data directory
â”‚   â”‚   â”œâ”€â”€ compacted  # Contains datasets in single jsonl files
â”‚   â”‚   â””â”€â”€ expanded  # Contains expanded datasets. Much easier to read and modify vs the jsonl files
â”‚   â”œâ”€â”€ evaluated  # Datasets that contain generated completions and corresponding evaluation results
â”‚   â””â”€â”€ generated  # Datasets that contain generated completions
â”œâ”€â”€ src  # Source code for ada-eval
â”œâ”€â”€ tests  # Tests for ada-eval
â”œâ”€â”€ tools  # Tools used by ada-eval during generation
â”œâ”€â”€ Makefile  # Contains various commands for convenience
â”œâ”€â”€ pyproject.toml  # Defines project dependencies and configuration
â”œâ”€â”€ README.md  # This file
â””â”€â”€ uv.lock  # Lock file for dependencies managed by uv
```

### Common Commands

To format your code, run:
```sh
make format
```

To run linter/type checker, run:

```sh
make check
```

To run the testsuite run:
```sh
make test
```

### Adding a new Sample (Challenge/Problem)

In `data/base/expanded`, you will find directories that each represent a dataset, which contain a number of samples (challenges/problems). The sample will have a different structure depending on the type of the dataset. Currently we only have SPARK datasets, though there is some support for Explain and Ada datasets too.

The general idea of each of these types are:
- **Ada**: Will require the modification of some Ada code to solve the challenge/problem
- **SPARK**: Will require the modification of some SPARK code to solve the challenge/problem. This could include adding new SPARK contracts, modifying existing ones, or changing the implementation to satisfy the given challenge.
- **Explain**: Contains some Ada/SPARK code and a question about it. The goal is to generate an explanation of the code that answers the question.

The rough structure of a SPARK sample is as follows:

```sh
example_sample
â”œâ”€â”€ base  # The project for the challenge. This will be duplicated somewhere for a tool to attempt to modify to solve the challenge
â”‚   â”œâ”€â”€ main.gpr
â”‚   â””â”€â”€ src
â”œâ”€â”€ comments.md  # Comments about the sample for humans to better understand it
â”œâ”€â”€ other.json  # Contains additional metadata for the sample. For SPARK samples, this will include the name of the subprogram of interest, and the relative path to the file that contains its specification.
â”œâ”€â”€ prompt.md  # The prompt that is provided to the tool to specify the challenge/problem
â”œâ”€â”€ solution  # A solution to the challenge/problem. For SPARK samples, this will be a project that can be built, passes the tests, and gnatprove is happy with.
â”‚   â”œâ”€â”€ main.gpr
â”‚   â”œâ”€â”€ main.adc
â”‚   â””â”€â”€ src
â””â”€â”€ tests  # Unit test project for the sample. Used in addition to gnatprove to verify that the solution is correct.
    â”œâ”€â”€ src
    â””â”€â”€ tests.gpr
```

#### Adding a new SPARK sample

1.  Create a new directory in `data/base/expanded/your_dataset_of_choice` with the name of the sample.
2.  Add the following:
    - `base/`: A project that contains the code that needs to be modified to solve the challenge/problem
    - `solution/`: A complete project that contains the modified code with the solved problem
    - `tests/`: A project that contains unit tests for the sample. It should compile to an executable at `tests/bin/tests` which returns a zero exit code if (and only if) the tests pass.
    - `comments.md`: if the challenge isn't obvious, consider including a more detailed description of the challenge/problem in this file
    - `prompt.md`: A prompt that describes the challenge/problem. This will be provided to the tool to generate a solution.
    - `other.json`: A file that contains additional metadata about the sample. For SPARK samples, this should look like:
      ```json
      {
          "location": {
              "path": "src/relative/path/to/file.ads",  // Should be relative to the base/solution directory of the sample
              "subprogram_name": "Name_Of_Subprogram"  // The name of the subprogram that is the focus of the challenge/problem
          }
      }
      ```
3.  Verify that the solution builds, proves, etc. by running
    ```sh
    make evaluate-canonical
    ```
    and checking the output in `other.json`.

### Generating new completions

To generate new completions for one or multiple datasets, you can use the `generate` command of the CLI. For example, to generate completions for all datasets using Claude Code, you can run:
```sh
uv run ada-eval generate --tool shell_script --tool-config-file tools/configs/claude_code_no_mcp.json
```
This also available via the shortcut `make generate-spark-claude`.

Currently you have to specify the type of tool you want to use, and the configuration file for that tool. The configuration files provide a place for modifying settings. Currently there are only shell tools, and the only values are used to specify where the script is located, and how long it should be allowed to run for.

This interface is not final.

### Evaluating completions

To evaluate completions that were generated as described in the previous section, you can run
```sh
uv run ada-eval evaluate
```
which will run all available evaluations on the generations in `data/generated/`, and save the results to `data/evaluated/`. This also available via the shortcut `make evaluate`.

For more options, such as specifying which evaluations are run, see the output of
```sh
uv run ada-eval evaluate --help
```

This interface is not final.


### Adding or Updating Python Dependencies

Since we are using our GitLab instance as our Python package registry (specifically `eng/ai/ada-eval` and `it/package-registry`), the following steps will depend on whether the desired package is already in the registry or not.

### The package version is already in `eng/ai/ada-eval` or `it/package-registry`

Run:

```sh
uv add <package-name>
uv sync
```

For more info see [docs](https://docs.astral.sh/uv/reference/cli/#uv-add)

### The package version is not in `eng/ai/rag` or `it/package-registry`

1. Find the new files that need to be added to our GitLab package registry
   1. Find the `tool.uv.index` section in `pyproject.toml` and either remove or comment out the line which says `default = true`
   2. Add the new package: `uv add <package-name>`, `uv lock`
   3. Find all of the URLS in `uv.lock` which start with `https://files.pythonhosted.org/packages`
2. Download copies of all files not in our package registry
   1. Make a temporary directory
   2. In that temporary directory, download all of the files from the URLs that start with `https://files.pythonhosted.org/packages`
      - This can be done by running `curl -L -O <url>` for each URL
3. Re-upload the files to our package registry

   1. Create or update ~/.pypirc to add the rag project to the index-servers:

      ```INI
      [distutils]
      index-servers =
          ada-eval

      [ada-eval]
      repository = https://gitlab.adacore-it.com/api/v4/projects/eng%2Fai%2Fada-eval/packages/pypi
      username: oauth2
      password: <token generated by e3-cli go, which can be found in ~/.adacore-jwts/gitlab_pat>
      ```

   2. Use twine to upload the files to the package registry: `uvx twine upload -r ada-eval --skip-existing *`
